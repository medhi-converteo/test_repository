{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " date :  2018-06-04\n",
      "---------01. starting query job\n",
      "job_errors =  None\n",
      "query_job =  DONE\n",
      "---------01. finished query job\n",
      "---------02. starting extracting job\n",
      "job_errors =  None\n",
      "extract_job =  DONE\n",
      "---------02. finished extracting job\n",
      "---------03. starting downloading_to_local job\n",
      "---------03. downloaded 72729485_users_20180604.csv from gcs to ../03_file_exports/temp_file.csv\n",
      "\n",
      "\n",
      "the whole time period has been processed\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#this notebook\n",
    "#1.get a start and end date\n",
    "#2.for each date\n",
    "    #1.retrieves the table ga_sessions in 3 suisses (specify account)\n",
    "    #2.transforms it (selects and renames columns)\n",
    "    #3.export it to gs\n",
    "\n",
    "#importing libraries\n",
    "from google.cloud import bigquery, storage\n",
    "from datetime import datetime,date, timedelta\n",
    "\n",
    "#getting today's date and time\n",
    "now = datetime.now()\n",
    "today_date_hour=now.strftime(\"%Y-%m-%d %H:%M\")\n",
    "#getting yesterday's date\n",
    "today_datetime=date.today()\n",
    "today=today_datetime.strftime('%Y%m%d')\n",
    "yesterday_datetime= (date.today() - timedelta(1))\n",
    "yesterday=yesterday_datetime.strftime('%Y%m%d')\n",
    "\n",
    "#start and end date\n",
    "#if you want to retrieve a history, replace start date by date(2018,5,20) for instance\n",
    "start_date=yesterday_datetime\n",
    "end_date=yesterday_datetime\n",
    "\n",
    "#params\n",
    "project_id='ringed-cell-652'\n",
    "    #input\n",
    "account_id = '72729485'\n",
    "dataset_id_original=account_id\n",
    "    #output\n",
    "dataset_id_transformed=account_id\n",
    "    #gs\n",
    "bucket_name='{}_users'.format(account_id)\n",
    "\n",
    "\n",
    "#creating clients with auth keys\n",
    "service_account_file='./../02_gcs_access/3Suisses GAP-9a0d1d5296de.json'\n",
    "\n",
    "bigquery_client=bigquery.Client(project=project_id).from_service_account_json(service_account_file)\n",
    "storage_client=storage.Client('ringed-cell-652').from_service_account_json(service_account_file)\n",
    "\n",
    "\n",
    "def main_scheduler():\n",
    "\n",
    "    #launching process\n",
    "    d=start_date\n",
    "    while d <= end_date:\n",
    "        print ('\\n\\n date : ',d)\n",
    "        d_suffix=d.strftime('%Y%m%d')\n",
    "        tranforming_table_in_bq(d_suffix)\n",
    "        exporting_to_gs(d_suffix)\n",
    "        downloading_to_local(d_suffix)\n",
    "        \n",
    "        #itÃ©ration sur la date\n",
    "        delta=timedelta(days=1)\n",
    "        d += delta\n",
    "        \n",
    "    print ('\\n\\nthe whole time period has been processed')\n",
    "\n",
    "\n",
    "def tranforming_table_in_bq(d_suffix):\n",
    "    \n",
    "    #table names changing depending on date\n",
    "    table_id_original='ga_sessions_{}'.format(d_suffix)\n",
    "    table_id_transformed='users_{}'.format(d_suffix)\n",
    "\n",
    "    #2.setting up configs\n",
    "        #configs\n",
    "    job_config=bigquery.QueryJobConfig()\n",
    "        #sql_query\n",
    "    sql_query='''\n",
    "    #standardSQL\n",
    "    CREATE OR REPLACE TABLE `dataset_id_transformed.table_id_transformed` AS\n",
    "      (\n",
    "      SELECT\n",
    "      fullvisitorId as NUMCLI,\n",
    "      visitId as NUMVSTWEB,\n",
    "      date as DATVSTWEB,\n",
    "      totals.timeOnSite as DURVSTWEBSEC,\n",
    "      totals.pageviews as NBRPAGVST\n",
    "\n",
    "      FROM\n",
    "      `project_id.dataset_id_original.table_id_original`\n",
    "      )\n",
    "\n",
    "    '''\n",
    "    sql_query=sql_query.replace(\n",
    "        'project_id',project_id).replace(\n",
    "        'dataset_id_original',dataset_id_original).replace(\n",
    "        'dataset_id_transformed',dataset_id_transformed).replace(\n",
    "        'table_id_transformed',table_id_transformed).replace(\n",
    "        'table_id_original',table_id_original)\n",
    "\n",
    "    #3.launching job\n",
    "    query_job=bigquery_client.query(\n",
    "    sql_query,\n",
    "    job_config\n",
    "    )\n",
    "    #print starting query job\n",
    "    print ('---------01. starting query job')\n",
    "\n",
    "    #4.checking errors\n",
    "        #wait for results\n",
    "    query_job.result()\n",
    "        #checking errors\n",
    "    print ('job_errors = ', query_job.errors)\n",
    "        #asserting job state is done\n",
    "    assert query_job.state == 'DONE'\n",
    "    print ('query_job = ',query_job.state)\n",
    "    print ('---------01. finished query job')\n",
    "    \n",
    "    return(table_id_original,table_id_transformed)\n",
    "    \n",
    "def exporting_to_gs(d_suffix):\n",
    "    #2.setting up configs\n",
    "    #inputs \n",
    "    dataset_id=dataset_id_transformed\n",
    "    dataset_ref=bigquery_client.dataset(dataset_id)\n",
    "    table_id='users_{}'.format(d_suffix)\n",
    "    table_ref=dataset_ref.table(table_id)\n",
    "    \n",
    "    \n",
    "    #outputs\n",
    "    file_name='{}_{}.csv'.format(dataset_id_original,table_id)\n",
    "    destination_uri='gs://{}/{}'.format(bucket_name,file_name)\n",
    "    #configs\n",
    "    job_config=bigquery.ExtractJobConfig()\n",
    "    job_config.compression=True\n",
    "\n",
    "    #3.launching job\n",
    "    extract_job=bigquery_client.extract_table(\n",
    "    table_ref,\n",
    "    destination_uri,\n",
    "    job_config=job_config)\n",
    "    #print starting extracting job\n",
    "    print ('---------02. starting extracting job')\n",
    "\n",
    "    #4.checking errors\n",
    "        #wait for results\n",
    "    extract_job.result()\n",
    "        #checking errors\n",
    "    print ('job_errors = ', extract_job.errors)\n",
    "        #asserting job state is done\n",
    "    assert extract_job.state == 'DONE'\n",
    "    print ('extract_job = ',extract_job.state)\n",
    "    print ('---------02. finished extracting job')\n",
    "\n",
    "def downloading_to_local(d_suffix):\n",
    "    #getting the desired file\n",
    "    bucket=storage_client.get_bucket(bucket_name)\n",
    "    file_to_download='{}_{}.csv'.format(bucket_name,d_suffix)\n",
    "    blob=bucket.blob(file_to_download)\n",
    "\n",
    "    #downloading the file to local temporary file\n",
    "    print ('---------03. starting downloading_to_local job')\n",
    "    temporary_file='../03_file_exports/temp_file.csv'\n",
    "    blob.download_to_filename(temporary_file)\n",
    "    print('---------03. downloaded %s from gcs to %s'%(file_to_download,temporary_file))\n",
    " \n",
    "    \n",
    "if __name__=='__main__':\n",
    "    main_scheduler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3Suisses GAP-9a0d1d5296de.json']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
